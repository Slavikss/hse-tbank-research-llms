data:
  train_path: data/train/train.jsonl
  eval_dir: data/eval
  prediction_dir: results/predictions

models:
  baseline: Qwen/Qwen2.5-1.5B-Instruct
  trained: outputs/merged_model

model:
  base_model: Qwen/Qwen2.5-1.5B-Instruct
  max_seq_length: 512
  load_in_4bit: true
  lora_r: 16
  lora_alpha: 32
  lora_dropout: 0.0
  target_modules:
    - q_proj
    - k_proj
    - v_proj
    - o_proj
    - gate_proj
    - up_proj
    - down_proj

training:
  output_dir: outputs/grpo_runs
  max_steps: 800
  learning_rate: 5.0e-6
  per_device_train_batch_size: 2
  gradient_accumulation_steps: 8
  num_generations: 4
  max_prompt_length: 480
  max_completion_length: 32
  mask_truncated_completions: true
  logging_steps: 10
  save_steps: 100
  seed: 2026

output:
  adapter_dir: outputs/adapter
  merged_dir: outputs/merged_model
  save_merged: true

inference:
  temperature: 0.0
  top_p: 1.0
  max_tokens: 96

dry_run:
  num_examples: 16
